{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.chdir('D:\\instacart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_train_df = pd.read_csv(\"order_products__train.csv\")\n",
    "order_products_prior_df = pd.read_csv(\"order_products__prior.csv\")\n",
    "orders_df = pd.read_csv(\"orders.csv\")\n",
    "products_df = pd.read_csv(\"products.csv\")\n",
    "aisles_df = pd.read_csv(\"aisles.csv\")\n",
    "departments_df = pd.read_csv(\"departments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orders_df.shape)\n",
    "orders_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing data\n",
    "\n",
    "add_to_cart_order - If an order has multiple items, then, there is a sequence for those items.\n",
    "\n",
    "\"prior\", \"train\" and \"test\" order - For each user_id, he/she have multiple orders, say n. The first n-1 orders are called prior, the last order of this user is either called train or test depending on if this user is in train data or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_prior_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[orders_df.user_id ==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_prior_df = pd.merge(order_products_prior_df,products_df,on='product_id',how='left')\n",
    "order_products_prior_df = pd.merge(order_products_prior_df,aisles_df,on='aisle_id',how='left')\n",
    "order_products_prior_df = pd.merge(order_products_prior_df,departments_df,on='department_id',how='left')\n",
    "order_products_prior_df = pd.merge(order_products_prior_df,orders_df,on='order_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting top few aisles based on % of purchases covered\n",
    "\n",
    "Based on my analysis below, we can see that the top 50 aisles out of the total of 134 aisles account for about 85% of the total transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## How many products of each aisle are sold?\n",
    "\n",
    "# Take row count for each aisle, sorted by largest to smallest count\n",
    "aisle_counts = pd.DataFrame(order_products_prior_df.groupby(['aisle_id', 'aisle']).order_id.count()).sort_values(by ='order_id',ascending=False).rename(columns={'order_id':'purchase_cnt'})\n",
    "\n",
    "# Calculate % of all purchases for each aisle\n",
    "aisle_counts['purchase_%'] = aisle_counts['purchase_cnt']*100/order_products_prior_df.order_id.count()\n",
    "\n",
    "# Calculate cumulative percentage of transactions at each row\n",
    "aisle_counts['cum_purchase_%'] = aisle_counts['purchase_%'].cumsum()\n",
    "\n",
    "# Formatting\n",
    "aisle_counts = aisle_counts.reset_index()\n",
    "\n",
    "# Preview\n",
    "aisle_counts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking top 50 rows to perform PCA\n",
    "\n",
    "# Number 50 decided after eyeballing results shown above\n",
    "aisles_counts_top_n = aisle_counts.head(50)\n",
    "\n",
    "# Preview\n",
    "aisles_counts_top_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualising results\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "ax = aisles_counts_top_n[['aisle', 'cum_purchase_%']].plot(x='aisle', linestyle='-', marker='o', label = 'Cummulative purchase %')\n",
    "ax = aisles_counts_top_n[['aisle', 'purchase_%']].plot(x='aisle', kind='bar', ax=ax, use_index = True, label = 'Purchase %')\n",
    "ax.legend([\"Cummulative purchase %\", \"Purchase % by aisle\"]);\n",
    "plt.xlabel('Aisle')\n",
    "plt.ylabel('% of Purchases')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many unique orders does each aisle occur in? - aisle penetration\n",
    "\n",
    "# Calculating #unique orders for each aisle\n",
    "aisle_penetration = pd.DataFrame(order_products_prior_df.groupby(['aisle_id', 'aisle']).order_id.nunique()).sort_values(by ='order_id',ascending=False).rename(columns={'order_id':'unique_orders'})\n",
    "\n",
    "# Calculating aisle penetration\n",
    "aisle_penetration['aisle_penetration'] = aisle_penetration['unique_orders']*100/order_products_prior_df['order_id'].nunique()\n",
    "\n",
    "# Formatting\n",
    "aisle_penetration = aisle_penetration.reset_index()\n",
    "\n",
    "# Preview\n",
    "aisle_penetration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking top 50 aisles by aisle penetration\n",
    "\n",
    "aisle_penetration_top_n = pd.DataFrame(aisle_penetration[:50]['aisle_id'])\n",
    "aisle_penetration_top_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## How many of the top aisles by purchase % are in top 50 by penetration?\n",
    "\n",
    "print(\"Number of aisles that are common in the top 50 by penetration and product purchase counts:\", aisles_counts_top_n.merge(aisle_penetration_top_n, how = 'inner').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Segmentation\n",
    "\n",
    "## We will observe customer behavior in these top 50 aisles and use that behavior to reduce the number of dimensions for clustering using PCA.\n",
    "\n",
    "## Clustering in high dimensional spaces becomes difficult since the notion of distance starts changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Creating filtered base data with transactions only for the top n \n",
    "\n",
    "order_products_prior_df_top_n_aisles = order_products_prior_df.merge(aisle_penetration_top_n['aisle_id'], on='aisle_id', how = 'inner')\n",
    "print(\"Shape of base data:\", order_products_prior_df_top_n_aisles.shape)\n",
    "order_products_prior_df_top_n_aisles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with count of all the purchases made by each user - by Aisle\n",
    "\n",
    "cust_aisle_top_n_aisles = pd.crosstab(order_products_prior_df_top_n_aisles.user_id, order_products_prior_df_top_n_aisles.aisle)\n",
    "cust_aisle_top_n_aisles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## List of top 50 aisles\n",
    "\n",
    "cust_aisle_top_n_aisles.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running PCA on aisle-counts\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_top_n = PCA(n_components=2)\n",
    "pca_top_n.fit(cust_aisle_top_n_aisles)\n",
    "pca_top_n_samples = pca_top_n.transform(cust_aisle_top_n_aisles)\n",
    "print(\"Ratio of variance explained by each PC:\",pca_top_n.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting PC1 and PC2 co-ordinates of each feature\n",
    "\n",
    "pca_top_n_comp_df = pd.DataFrame(pca_top_n.components_)\n",
    "pca_top_n_comp_df.columns = cust_aisle_top_n_aisles.columns\n",
    "pca_top_n_comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Crearting DF to plot these features in the PC space\n",
    "\n",
    "pca_top_n_comp_df_plt = pca_top_n_comp_df.T.reset_index()\n",
    "pca_top_n_comp_df_plt.columns = ['aisle_name', 'pc1', 'pc2']\n",
    "\n",
    "pca_top_n_comp_df_plt['pc1'] = pca_top_n_comp_df_plt['pc1']*100\n",
    "pca_top_n_comp_df_plt['pc2'] = pca_top_n_comp_df_plt['pc2']*100\n",
    "pca_top_n_comp_df_plt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,14))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "for i,pca_name in enumerate(pca_top_n_comp_df_plt['aisle_name']):\n",
    "    x = pca_top_n_comp_df_plt['pc1'][i]\n",
    "    y = pca_top_n_comp_df_plt['pc2'][i]\n",
    "    plt.scatter(x, y, marker='x', color='red')\n",
    "    plt.text(x+0.3, y+0.3, pca_name, fontsize=9)\n",
    " \n",
    "plt.suptitle('Aisles in PCA space', y=0.9, size=16)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cleaner plot\n",
    "As expected, the top 4 aisles behave very differently vs. all other aisles. For a closer look at the other features, we plot without them in the plot.\n",
    "\n",
    "PC1 seems to capture information related to __ - actual penetration decreases as we move towards origin along PC1\n",
    "\n",
    "What does PC2 capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pca_top_n_comp_df_plt_treated = pca_top_n_comp_df_plt[(pca_top_n_comp_df_plt['aisle_name']!='yogurt') & (pca_top_n_comp_df_plt['aisle_name']!='fresh fruits') & (pca_top_n_comp_df_plt['aisle_name']!='fresh vegetables') & (pca_top_n_comp_df_plt['aisle_name']!='packaged vegetables fruits')].reset_index(drop=True)\n",
    "fig = plt.figure(figsize=(15,14))\n",
    "\n",
    "for i,pca_name in enumerate(pca_top_n_comp_df_plt_treated['aisle_name']):\n",
    "    x = pca_top_n_comp_df_plt_treated['pc1'][i]\n",
    "    y = pca_top_n_comp_df_plt_treated['pc2'][i]\n",
    "    plt.scatter(x, y, marker='x', color='red')\n",
    "    plt.text(x+0.05, y+0.05, pca_name, fontsize=9)\n",
    " \n",
    "plt.suptitle('Aisles in PCA space - Closer look', y=0.9, size=16)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing X and Y co-ordinates of data points (users) in PCA space\n",
    "\n",
    "ps_top_n = pd.DataFrame(pca_top_n_samples)\n",
    "ps_top_n.head()\n",
    "tocluster_top_n = pd.DataFrame(ps_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running K-means on all customers who transacted in top n aisles (data points)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=3,random_state=42)\n",
    "kmeans.fit(tocluster_top_n)\n",
    "labels_top_n = kmeans.predict(tocluster_top_n)\n",
    "centers_top_n = kmeans.cluster_centers_\n",
    "print(centers_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding cluster information for all users\n",
    "\n",
    "ps_top_n['cluster']=labels_top_n\n",
    "ps_top_n['user_id'] = cust_aisle_top_n_aisles.index\n",
    "ps_top_n = ps_top_n[['user_id',0,1,'cluster']]\n",
    "ps_top_n.columns = ['user_id','PC1','PC2','cluster']\n",
    "ps_top_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Plotting all users colored by clusters\n",
    "\n",
    "fig = plt.figure(figsize=(15,14))\n",
    "\n",
    "plt.scatter(ps_top_n['PC1'],ps_top_n['PC2'], marker='o', c=ps_top_n['cluster'])\n",
    "\n",
    "plt.suptitle('Customers in PCA space', y=0.9, size=16)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating user level metrics¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating average order gap for each user\n",
    "\n",
    "user_days_prior = pd.DataFrame(order_products_prior_df[['user_id','order_id','days_since_prior_order']].drop_duplicates())\n",
    "user_days_prior = pd.DataFrame(user_days_prior.groupby('user_id')['days_since_prior_order'].mean()).reset_index()\n",
    "user_days_prior.columns = ['user_id','avg_order_gap']\n",
    "user_days_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining user level info\n",
    "\n",
    "## Order and purchase information\n",
    "user_info = pd.DataFrame(order_products_prior_df_top_n_aisles.groupby('user_id')['order_id'].agg(['count','nunique'])).reset_index()\n",
    "user_info.columns = ['user_id','purchase_count','order_count']\n",
    "\n",
    "## Average order gap\n",
    "user_info = user_info.merge(user_days_prior, on = 'user_id', how = 'inner')\n",
    "\n",
    "## Average basket size\n",
    "user_info['avg_basket_size'] = user_info['purchase_count']/user_info['order_count']\n",
    "\n",
    "## Cluster information\n",
    "user_info = user_info.merge(ps_top_n[['user_id','cluster']], on = 'user_id', how = 'inner')\n",
    "\n",
    "\n",
    "user_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing percentage of users which are one-time transactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info[user_info['order_count']==1].count()*100/user_info[user_info['cluster']==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### We see that these customers are 1.5% of our low transacting customers, which is negligible. We will not consider them as a separate cluster.\n",
    "\n",
    "### Instead, we will add a one_n_done flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding one_n_done flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info['one_n_done'] = np.where(user_info['order_count']==1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster level metrics¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall segments\n",
    "\n",
    "cluster_info = user_info.groupby('cluster')['avg_basket_size','order_count','purchase_count','avg_order_gap'].agg({'avg_order_gap':['mean','median'],'avg_basket_size':['mean','median','count'],'purchase_count':['sum'],'order_count':['mean','median','sum']})\n",
    "cluster_info.columns = cluster_info.columns.map('_'.join)\n",
    "cluster_info['perc_cust_in_clust'] = cluster_info['avg_basket_size_count']*100/cluster_info['avg_basket_size_count'].sum(axis = 0)\n",
    "cluster_info['perc_order_from_clust'] = cluster_info['order_count_sum']*100/cluster_info['order_count_sum'].sum(axis = 0)\n",
    "cluster_info['perc_purch_from_clust'] = cluster_info['purchase_count_sum']*100/cluster_info['purchase_count_sum'].sum(axis = 0)\n",
    "cluster_info = cluster_info.reset_index()\n",
    "cluster_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster level metric list:¶\n",
    "1. avg order gap (mean) - in days\n",
    "2. avg order gap (median) - in days\n",
    "3. avg basket size (mean) - number of unique items/basket on an average (signals variety rather than actual basket size)\n",
    "4. avg basket size (median) - number of unique items/basket on an average (signals variety rather than actual basket size)\n",
    "5. avg basket size (count) - number of rows ==> proxy for number of users in cluster\n",
    "6. purchase count (sum) - total number of items purchased by customers in cluster\n",
    "7. order count (mean)- average number of orders for customers of cluster\n",
    "8. order count (median)- median number of orders for customers of cluster\n",
    "9. order count (sum)- total number of orders for customers of cluster\n",
    "10. perc cust in clust - total percentage of customers in cluster\n",
    "11. perc order from clust - total percentage of orders from cluster\n",
    "12. perc purch from clust - total percentage of purchases from cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including one_n_done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall segments\n",
    "\n",
    "cluster_info_one_n_done = user_info.groupby(['cluster','one_n_done'])['avg_basket_size','order_count','purchase_count'].agg({'avg_basket_size':['mean','median','count'],'purchase_count':['mean','median','sum'],'order_count':['mean','median','sum']})\n",
    "cluster_info_one_n_done.columns = cluster_info_one_n_done.columns.map('_'.join)\n",
    "cluster_info_one_n_done['perc_cust_in_clust'] = cluster_info_one_n_done['avg_basket_size_count']*100/cluster_info_one_n_done['avg_basket_size_count'].sum(axis = 0)\n",
    "cluster_info_one_n_done['perc_order_from_clust'] = cluster_info_one_n_done['order_count_sum']*100/cluster_info_one_n_done['order_count_sum'].sum(axis = 0)\n",
    "cluster_info_one_n_done['perc_purch_from_clust'] = cluster_info_one_n_done['purchase_count_sum']*100/cluster_info_one_n_done['purchase_count_sum'].sum(axis = 0)\n",
    "cluster_info_one_n_done = cluster_info_one_n_done.reset_index()\n",
    "cluster_info_one_n_done.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aisle penetrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_prior_df = order_products_prior_df.merge(user_info[['user_id','cluster']], on = 'user_id', how = 'inner')\n",
    "order_products_prior_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Calculator - Return pairs of products/aisles with high lift¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "def one_hot_encoder(df, antecendent_list, ideal_cluster):\n",
    "    \n",
    "    ## df: dataframe to be used\n",
    "    ## id_col: which column to use for unique identifiers of frequent items (aisle, product)\n",
    "    ## name_col: which column to use for names of frequent_items\n",
    "    ## antecedent list: list of objects to be used as antecedents\n",
    "    \n",
    "    trans_df = df\n",
    "    \n",
    "    ## Filtering transactions only for target cluster and ideal cluster\n",
    "    trans_df = trans_df[(trans_df['cluster']== ideal_cluster)]\n",
    "    \n",
    "    ## Selecting transactions with antecedent aisles present\n",
    "    order_list_base = trans_df[(trans_df['aisle_id'].isin(antecendent_list))]\n",
    "    order_list = pd.DataFrame(order_list_base['order_id'].unique())\n",
    "    order_list.columns=['order_id']\n",
    "    \n",
    "    ## Filtered transactions for antecedent list\n",
    "    trans_df_filtered = trans_df.merge(order_list, on='order_id', how='inner')\n",
    "    \n",
    "    orders = order_list['order_id'].tolist()\n",
    "    transaction_aisles = []\n",
    "    \n",
    "    for order in orders:\n",
    "        tmp_df = trans_df_filtered[trans_df_filtered['order_id']==order]\n",
    "        aisles_tmp = tmp_df['aisle'].unique().tolist()\n",
    "        transaction_aisles.append(aisles_tmp)\n",
    "    \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transaction_aisles).transform(transaction_aisles)\n",
    "    trans_onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    return trans_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "def one_hot_encoder_overall(df, ideal_cluster):\n",
    "    \n",
    "    ## df: dataframe to be used\n",
    "    ## id_col: which column to use for unique identifiers of frequent items (aisle, product)\n",
    "    ## name_col: which column to use for names of frequent_items\n",
    "    ## antecedent list: list of objects to be used as antecedents\n",
    "    \n",
    "    trans_df = df\n",
    "    \n",
    "    ## Filtering transactions only for target cluster and ideal cluster\n",
    "    trans_df = trans_df[(trans_df['cluster']== ideal_cluster)]\n",
    "   \n",
    "    orders = trans_df['order_id'].tolist()\n",
    "    transaction_aisles = []\n",
    "    \n",
    "    for order in orders:\n",
    "        tmp_df = trans_df[trans_df['order_id']==order]\n",
    "        aisles_tmp = tmp_df['aisle'].unique().tolist()\n",
    "        transaction_aisles.append(aisles_tmp)\n",
    "    \n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transaction_aisles).transform(transaction_aisles)\n",
    "    trans_onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    return trans_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming transactions to 1-hot\n",
    "\n",
    "#order_products_prior_df = order_products_prior_df.merge(user_info[['user_id','cluster']], on = 'user_id', how = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_transactions_cluster_1 = one_hot_encoder_overall(order_products_prior_df,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating inter-aisle, aisle level lifts\n",
    "\n",
    "frequent_itemsets_aisle_cluster_1 = apriori(encoded_transactions_cluster_1, min_support=0.2, use_colnames=True)\n",
    "aisle_association_rules_df_cluster_1 = association_rules(frequent_itemsets_aisle_cluster_1, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisle_association_rules_df_cluster_1.to_csv('aisle_association_rules_df_cluster_1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming transactions to 1-hot\n",
    "\n",
    "encoded_transactions_cluster_2 = one_hot_encoder_overall(order_products_prior_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Calculating inter-aisle, aisle level lifts\n",
    "\n",
    "frequent_itemsets_aisle_cluster_2 = apriori(encoded_transactions_cluster_2, min_support=0.2, use_colnames=True)\n",
    "aisle_association_rules_df_cluster_2 = association_rules(frequent_itemsets_aisle_cluster_2, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aisle_association_rules_df_cluster_2.to_csv('aisle_association_rules_df_cluster_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
